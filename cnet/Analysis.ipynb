{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main training script for Cascaded Nets.\"\"\"\n",
    "import argparse\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from datasets.dataset_handler import DataHandler\n",
    "from matplotlib.lines import Line2D\n",
    "from modules import utils\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-electric",
   "metadata": {},
   "source": [
    "## Setup Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_args():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--random_seed\", type=int, default=42,\n",
    "                      help=\"random seed\")\n",
    "  \n",
    "  # Paths\n",
    "  parser.add_argument(\"--experiment_root\", type=str, \n",
    "                      default='experiments',\n",
    "                      help=\"Local output dir\")\n",
    "#   parser.add_argument(\"--experiment_name\", type=str, \n",
    "#                       required=True,\n",
    "#                       help=\"Experiment name\")\n",
    "  \n",
    "  # Dataset\n",
    "#   parser.add_argument(\"--dataset_root\", type=str, required=True,\n",
    "#                       help=\"Dataset root\")\n",
    "#   parser.add_argument(\"--dataset_name\", type=str, required=True,\n",
    "#                       help=\"Dataset name: CIFAR10, CIFAR100, TinyImageNet\")\n",
    "  parser.add_argument(\"--split_idxs_root\", type=str, default='split_idxs',\n",
    "                      help=\"Split idxs root\")\n",
    "  parser.add_argument(\"--val_split\", type=float, default=0.1,\n",
    "                      help=\"Validation set split: 0.1 default\")\n",
    "  parser.add_argument(\"--augmentation_noise_type\", type=str, \n",
    "                      default='occlusion',\n",
    "                      help=\"Augmentation noise type: occlusion\")\n",
    "  parser.add_argument(\"--batch_size\", type=int, default=128,\n",
    "                      help=\"batch_size\")\n",
    "  parser.add_argument(\"--num_workers\", type=int, default=2,\n",
    "                      help=\"num_workers\")\n",
    "  parser.add_argument('--drop_last', action='store_true', default=False,\n",
    "                      help='Drop last batch remainder')\n",
    "  \n",
    "  # Model\n",
    "  parser.add_argument(\"--model_key\", type=str, default='resnet18',\n",
    "                      help=\"Model: resnet18, resnet34, ..., densenet_cifar\")\n",
    "  parser.add_argument(\"--train_mode\", type=str, \n",
    "                      default='baseline',\n",
    "                      help=\"Train mode: baseline, ic_only, sdn\")\n",
    "  parser.add_argument('--bn_time_affine', action='store_true', default=False,\n",
    "                      help='Use temporal affine transforms in BatchNorm')\n",
    "  parser.add_argument('--bn_time_stats', action='store_true', default=False,\n",
    "                      help='Use temporal stats in BatchNorm')\n",
    "  parser.add_argument(\"--tdl_mode\", type=str, \n",
    "                      default='OSD',\n",
    "                      help=\"TDL mode: OSD, EWS, noise\")\n",
    "  parser.add_argument(\"--tdl_alpha\", type=float, default=0.0,\n",
    "                      help=\"TDL alpha for EWS temporal kernel\")\n",
    "  parser.add_argument(\"--noise_var\", type=float, default=0.0,\n",
    "                      help=\"Noise variance on noise temporal kernel\")\n",
    "  parser.add_argument(\"--lambda_val\", type=float, default=1.0,\n",
    "                      help=\"TD lambda value\")\n",
    "  parser.add_argument('--cascaded', action='store_true', default=False,\n",
    "                      help='Cascaded net')\n",
    "  parser.add_argument(\"--cascaded_scheme\", type=str, default='parallel',\n",
    "                      help=\"cascaded_scheme: serial, parallel\")\n",
    "  parser.add_argument(\"--init_tau\", type=float, default=0.01,\n",
    "                      help=\"Initial tau valu\")\n",
    "  parser.add_argument(\"--target_IC_inference_costs\", nargs=\"+\", type=float, \n",
    "                      default=[0.15, 0.30, 0.45, 0.60, 0.75, 0.90],\n",
    "                      help=\"target_IC_inference_costs\")\n",
    "  parser.add_argument('--tau_weighted_loss', action='store_true', default=False,\n",
    "                      help='Use tau weights on IC losses')\n",
    "  \n",
    "  \n",
    "  # Optimizer\n",
    "  parser.add_argument(\"--learning_rate\", type=float, default=0.1,\n",
    "                      help=\"learning rate\")\n",
    "  parser.add_argument(\"--momentum\", type=float, default=0.9,\n",
    "                      help=\"momentum\")\n",
    "  parser.add_argument(\"--weight_decay\", type=float, default=0.0005,\n",
    "                      help=\"weight_decay\")\n",
    "  parser.add_argument('--nesterov', action='store_true', default=False,\n",
    "                      help='Nesterov for SGD')\n",
    "  parser.add_argument('--normalize_loss', action='store_true', default=False,\n",
    "                      help='Normalize temporal loss')\n",
    "  \n",
    "  # LR scheduler\n",
    "  parser.add_argument(\"--lr_milestones\", nargs=\"+\", type=float, \n",
    "                      default=[60, 120, 150],\n",
    "                      help=\"lr_milestones\")\n",
    "  parser.add_argument(\"--lr_schedule_gamma\", type=float, default=0.2,\n",
    "                      help=\"lr_schedule_gamma\")\n",
    "  \n",
    "  # Other\n",
    "  parser.add_argument('--use_cpu', action='store_true', default=False,\n",
    "                      help='Use cpu')\n",
    "  parser.add_argument(\"--device\", type=int, default=0,\n",
    "                      help=\"GPU device num\")\n",
    "  parser.add_argument(\"--n_epochs\", type=int, default=150,\n",
    "                      help=\"Number of epochs to train\")\n",
    "  parser.add_argument(\"--eval_freq\", type=int, default=10,\n",
    "                      help=\"eval_freq\")\n",
    "  parser.add_argument(\"--save_freq\", type=int, default=5,\n",
    "                      help=\"save_freq\")\n",
    "  parser.add_argument('--keep_logits', action='store_true', default=False,\n",
    "                      help='Keep logits')\n",
    "  parser.add_argument('--debug', action='store_true', default=False,\n",
    "                      help='Debug mode')\n",
    "  \n",
    "  args = parser.parse_args(\"\")\n",
    "  \n",
    "  # Flag check\n",
    "  if args.tdl_mode == 'EWS':\n",
    "    assert args.tdl_alpha is not None, 'tdl_alpha not set'\n",
    "  elif args.tdl_mode == 'noise':\n",
    "    assert args.noise_var is not None, 'noise_var not set'\n",
    "    \n",
    "  return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = setup_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-transcription",
   "metadata": {},
   "source": [
    "## Override Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_root = '/hdd/mliuzzolino/datasets'\n",
    "args.experiment_root = '/home/mliuzzolino/experiment_output'\n",
    "  \n",
    "# Set required flags|\n",
    "args.dataset_name = 'ImageNet2012'  # CIFAR10, CIFAR100, ImageNet2012\n",
    "args.model_key = 'resnet18'\n",
    "args.dataset_key = 'test'  # val, test\n",
    "if args.dataset_name == \"ImageNet2012\":\n",
    "  args.experiment_name = f\"{args.model_key}_{args.dataset_name}\"\n",
    "elif \"cifar\" in args.dataset_name.lower():\n",
    "  args.experiment_name = f\"{args.model_key}_{args.dataset_name.lower()}\"\n",
    "else:\n",
    "  print(\"TinyImageNet not implemented yet!\")\n",
    "  \n",
    "args.val_split = 0.1\n",
    "args.test_split = 0.1\n",
    "args.split_idxs_root = \"/hdd/mliuzzolino/split_idxs\"\n",
    "args.tdl_mode = 'OSD'  # OSD, EWS\n",
    "args.tau_weighted_loss = True\n",
    "args.random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make reproducible\n",
    "utils.make_reproducible(args.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-authority",
   "metadata": {},
   "source": [
    "## Setup Figs Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_root = 'figs'\n",
    "if not os.path.exists(figs_root):\n",
    "  os.makedirs(figs_root)\n",
    "  \n",
    "fig_path = os.path.join(figs_root, f'{args.dataset_name}.pdf')\n",
    "fig_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-unknown",
   "metadata": {},
   "source": [
    "## Model Label Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LBL_LOOKUP = {\n",
    "    \"cascaded__serial\": \"SerialTD\",\n",
    "    \"cascaded__parallel\": \"CascadedTD\",\n",
    "    \"cascaded__serial__multiple_fcs\": \"SerialTD-MultiHead\",  # (SDN)\n",
    "    \"cascaded__parallel__multiple_fcs\": \"CascadedTD-MultiHead\",\n",
    "    \"cascaded_seq__serial\": \"SerialCE\",\n",
    "    \"cascaded_seq__parallel\": \"CascadedCE\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-conservative",
   "metadata": {},
   "source": [
    "## Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_src = {\n",
    "    \"CascadedTDColor\": np.array([182,54,121]) / 255.0,  # CascadedTDColor,\n",
    "    \"CascadedCEColor\": np.array([127,38,110]) / 255.0,  # CascadedCEColor,\n",
    "    \"SerialTDColor\": np.array([77,184,255]) / 255.0,  # SerialTDColor,\n",
    "    \"SerialCEColor\": np.array([54,129,179]) / 255.0,  # SerialCEColor,\n",
    "}\n",
    "\n",
    "model_colors = {\n",
    "    \"cascaded__serial\": colors_src[\"SerialTDColor\"],\n",
    "    \"cascaded__parallel\": colors_src[\"CascadedTDColor\"],\n",
    "    \"cascaded__serial__multiple_fcs\": colors_src[\"SerialTDColor\"],  # (SDN)\n",
    "    \"cascaded__parallel__multiple_fcs\": colors_src[\"CascadedTDColor\"],\n",
    "    \"cascaded_seq__serial\": colors_src[\"SerialCEColor\"],\n",
    "    \"cascaded_seq__parallel\": colors_src[\"CascadedCEColor\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-blowing",
   "metadata": {},
   "source": [
    "## Setup Data Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handler\n",
    "data_dict = {\n",
    "    \"dataset_name\": args.dataset_name,\n",
    "    \"data_root\": args.dataset_root,\n",
    "    \"val_split\": args.val_split,\n",
    "    \"test_split\": args.test_split,\n",
    "    \"split_idxs_root\": args.split_idxs_root,\n",
    "    \"noise_type\": args.augmentation_noise_type,\n",
    "    \"load_previous_splits\": True,\n",
    "    \"imagenet_params\": {\n",
    "      \"target_classes\": [\"terrier\"],\n",
    "      \"max_classes\": 10,\n",
    "    }\n",
    "}\n",
    "data_handler = DataHandler(**data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Loaders\n",
    "test_loader = data_handler.build_loader(args.dataset_key, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-blood",
   "metadata": {},
   "source": [
    "## Load Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment root\n",
    "exp_root = os.path.join(args.experiment_root,\n",
    "                        args.experiment_name,\n",
    "                        'experiments')\n",
    "\n",
    "exp_root = f\"/hdd/mliuzzolino/cascaded_nets/{args.experiment_name}/experiments\"\n",
    "\n",
    "# Find exp paths\n",
    "exp_paths = np.sort(glob.glob(f'{exp_root}/*/outputs/*__{args.dataset_key}__{args.tdl_mode}.pt'))\n",
    "print(f\"Num paths: {len(exp_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-translator",
   "metadata": {},
   "source": [
    "#### Build Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-penny",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dict = defaultdict(list)\n",
    "outrep_id = 0\n",
    "outreps_dict = {}\n",
    "ic_costs_lookup = {}\n",
    "exp_path_lookup = {}\n",
    "for i, exp_path in enumerate(exp_paths):\n",
    "  outrep_id = f'rep_id_{i}'\n",
    "  outrep = torch.load(exp_path)\n",
    "  \n",
    "  basename = [ele for ele in exp_path.split(os.path.sep) if 'seed_' in ele][0]\n",
    "  keys = basename.split(',')\n",
    "  if keys[0].startswith('std') or keys[0].startswith('cascaded_seq'):\n",
    "    model_key, lr, weight_decay, seed = keys\n",
    "    td_key = 'std'\n",
    "  else:\n",
    "    td_key, scheme_key, lr, weight_decay, seed = keys[:5]\n",
    "    model_key = f'cascaded__{scheme_key}'\n",
    "    other_keys = keys[5:]\n",
    "    multiple_fcs = 'multiple_fcs' in other_keys\n",
    "    tau_weighted = 'tau_weighted' in other_keys\n",
    "    pretrained_weights = 'pretrained_weights' in other_keys\n",
    "    if multiple_fcs:\n",
    "      model_key = f'{model_key}__multiple_fcs'\n",
    "    if tau_weighted:\n",
    "      model_key = f'{model_key}__tau_weighted'\n",
    "    if pretrained_weights:\n",
    "      model_key = f'{model_key}__pretrained_weights'\n",
    "\n",
    "  if model_key != 'std':\n",
    "    exp_root = os.path.dirname(os.path.dirname(exp_path))\n",
    "    IC_cost_path = os.path.join(exp_root, 'ic_costs.pt')\n",
    "    if os.path.exists(IC_cost_path):\n",
    "      IC_costs = torch.load(IC_cost_path)\n",
    "    else:\n",
    "      IC_costs = None\n",
    "  else:\n",
    "    IC_costs = None\n",
    "    \n",
    "  lr = float(lr.split(\"_\")[1])\n",
    "  weight_decay = float(weight_decay.split(\"_\")[1])\n",
    "\n",
    "  df_dict['model'].append(model_key)\n",
    "  df_dict['td_lambda'].append(td_key)\n",
    "  df_dict['lr'].append(lr)\n",
    "  df_dict['weight_decay'].append(weight_decay)\n",
    "  df_dict['seed'].append(seed)\n",
    "  df_dict['outrep_id'].append(outrep_id)\n",
    "  \n",
    "  outreps_dict[outrep_id] = outrep\n",
    "  ic_costs_lookup[outrep_id] = IC_costs\n",
    "  exp_path_lookup[outrep_id] = exp_path\n",
    "analysis_df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-expense",
   "metadata": {},
   "source": [
    "## Build Aggregate Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = defaultdict(list)\n",
    "analysis_df = analysis_df.sort_values('model')\n",
    "for i, df_i in analysis_df.iterrows():\n",
    "  outrep_i = outreps_dict[df_i.outrep_id]\n",
    "  accs = outrep_i['correct'].float().mean(dim=1)\n",
    "  \n",
    "  for i, acc in enumerate(accs):\n",
    "    df_dict['acc'].append(acc.item() * 100)\n",
    "    if len(accs) == 1:\n",
    "      i = -1\n",
    "    df_dict['ic'].append(i)\n",
    "    for k in list(df_i.index):\n",
    "      df_dict[k].append(df_i[k])\n",
    "accs_df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_df = accs_df.sort_values(['outrep_id', 'ic'])\n",
    "\n",
    "df_dict = defaultdict(list)\n",
    "for model_key, model_df in accs_df.groupby('model'):\n",
    "  for td_lambda, lambda_df in model_df.groupby('td_lambda'):\n",
    "    for ic, ic_df in lambda_df.groupby('ic'):\n",
    "      mean_acc = np.mean(ic_df.acc)\n",
    "      sem_acc = np.std(ic_df.acc) / np.sqrt(len(ic_df.acc))\n",
    "      outrep_id = ic_df.outrep_id.iloc[0]\n",
    "      df_dict['model'].append(model_key)\n",
    "      df_dict['td_lambda'].append(td_lambda)\n",
    "      df_dict['ic'].append(ic)\n",
    "      df_dict['acc'].append(mean_acc)\n",
    "      df_dict['sem'].append(sem_acc)\n",
    "      df_dict['outrep_id'].append(outrep_id)\n",
    "      \n",
    "      flops = ic_costs_lookup[outrep_id]\n",
    "      if flops is not None:\n",
    "        try:\n",
    "          flops = flops['flops'][ic]\n",
    "        except:\n",
    "          flops = 1.0\n",
    "      df_dict['flops'].append(flops)\n",
    "stats_df = pd.DataFrame(df_dict)\n",
    "stats_df.loc[stats_df.ic==-1, 'ic'] = np.max(stats_df.ic)\n",
    "stats_df.ic = [ele+1 for ele in stats_df.ic]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-conservation",
   "metadata": {},
   "source": [
    "## Accuracy vs. TD($\\lambda$) Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = np.array([182,54,121]) / 255.0\n",
    "\n",
    "AXIS_LBL_FONTSIZE = 16\n",
    "LEGEND_FONTSIZE = 16\n",
    "TICK_FONTSIZE = 14\n",
    "FIGSIZE = (8,4) # (12,5)\n",
    "\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "g = sns.lineplot(\n",
    "    x=\"td_lambda\", \n",
    "    y=\"acc\", \n",
    "    data=model_df,\n",
    "    lw=6,\n",
    "    color=color,\n",
    ")\n",
    "g = sns.scatterplot(\n",
    "    x=\"td_lambda\", \n",
    "    y=\"acc\", \n",
    "    data=model_df,\n",
    "    s=100,\n",
    "    color=color,\n",
    ")\n",
    "g.set_xticks(model_df.td_lambda.unique())\n",
    "g.set_xlabel(r\"$\\lambda$\", fontsize=AXIS_LBL_FONTSIZE)\n",
    "g.set_ylabel(\"Accuracy (%)\", fontsize=AXIS_LBL_FONTSIZE)\n",
    "g.tick_params(axis='x', labelsize=TICK_FONTSIZE)\n",
    "g.tick_params(axis='y', labelsize=TICK_FONTSIZE)\n",
    "save_path = os.path.join(\"figs\", f\"{args.dataset_name}__td_curves.pdf\")\n",
    "plt.savefig(save_path, dpi=300)\n",
    "plt.savefig(save_path.replace(\".pdf\", \".png\"), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-romantic",
   "metadata": {},
   "source": [
    "# Compute Speed Accuracy Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_threshold_conf_correct(pred_conf, correct, q):\n",
    "  correct_arr = []\n",
    "  best_clf_idxs = []\n",
    "  n_clfs = pred_conf.shape[0]\n",
    "  n_samples = pred_conf.shape[1]\n",
    "  for i in range(n_samples):\n",
    "    pred_conf_i = pred_conf[:,i]\n",
    "    idxs = np.where(pred_conf_i >= q)[0]\n",
    "    if not len(idxs):\n",
    "      best_clf_idx = n_clfs - 1\n",
    "      cor_i = correct[best_clf_idx,i]\n",
    "    else:\n",
    "      best_clf_idx = idxs[0]\n",
    "      cor_i = correct[best_clf_idx,i]\n",
    "    correct_arr.append(cor_i)\n",
    "    best_clf_idxs.append(best_clf_idx)\n",
    "  avg_acc = np.mean(correct_arr)\n",
    "  return avg_acc, best_clf_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = defaultdict(list)\n",
    "for model_key, model_df in analysis_df.groupby('model'):\n",
    "  if model_key == 'cascaded_seq__serial':\n",
    "    delta = 100\n",
    "  else:\n",
    "    delta = 100\n",
    "  Qs = np.linspace(0, 1, delta)\n",
    "  \n",
    "  if model_key in ['std']:\n",
    "    continue\n",
    "    \n",
    "  print(f\"\\nModel: {model_key}\")\n",
    "  for td_lambda, td_df in model_df.groupby('td_lambda'):\n",
    "    for jj, df_i in td_df.iterrows():\n",
    "      outrep_id = df_i.outrep_id\n",
    "      outrep_i = outreps_dict[outrep_id]\n",
    "      try:\n",
    "        flop_costs = ic_costs_lookup[outrep_id]['flops']\n",
    "      except:\n",
    "        for k, v in ic_costs_lookup.items():\n",
    "          if v is not None:\n",
    "            flop_costs = v['flops']\n",
    "            break\n",
    "      \n",
    "      f_interpolate = interpolate.interp1d(range(len(flop_costs)), flop_costs, bounds_error=False)\n",
    "\n",
    "      prediction_confidence = outrep_i['prediction_confidence']\n",
    "      correct_vals = outrep_i['correct']\n",
    "\n",
    "      accs = []\n",
    "      flops = []\n",
    "      for qi, q in enumerate(Qs):\n",
    "        sys.stdout.write((f'\\rTD_lambda: {td_lambda} -- '\n",
    "                          f'df_{jj} [{jj+1}/{len(td_df)}] -- '\n",
    "                          f'Threshold, q: {q:0.2f} [{qi+1}/{len(Qs)}]'))\n",
    "        sys.stdout.flush()\n",
    "        acc_i, best_clf_idxs = compute_threshold_conf_correct(prediction_confidence, correct_vals, q)\n",
    "        avg_timesteps = np.mean(best_clf_idxs)\n",
    "        avg_flop = f_interpolate(avg_timesteps)\n",
    "        \n",
    "        flops.append(avg_flop)\n",
    "        df_dict['model'].append(model_key)\n",
    "        df_dict['td_lambda'].append(td_lambda)\n",
    "        df_dict['seed'].append(df_i.seed)\n",
    "        df_dict['acc'].append(acc_i * 100)\n",
    "        df_dict['flops'].append(avg_flop)\n",
    "        df_dict['timesteps'].append(avg_timesteps)\n",
    "        df_dict['q'].append(q)\n",
    "      print(\"\\n\")\n",
    "speed_acc_data_df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-benefit",
   "metadata": {},
   "source": [
    "### Agg Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_df(df_src, fix_key='cascaded_seq__serial'):\n",
    "  df_src = df_src.copy()\n",
    "  df = df_src[df_src.model==fix_key]\n",
    "  df = df.sort_values('timestep_mean')\n",
    "  prev_j = None\n",
    "  for i, j in df.iterrows():\n",
    "    if j.timestep_mean != 0.0:\n",
    "      break\n",
    "    prev_j = j\n",
    "\n",
    "  for t in np.linspace(prev_j.timestep_mean, j.timestep_mean, 10)[:-1]:\n",
    "    new_j = prev_j.copy()\n",
    "    new_j.timestep_mean = t\n",
    "    new_j.timestep_mean = t\n",
    "    df = df.append(new_j, ignore_index=True)\n",
    "  df = df.sort_values('timestep_mean')\n",
    "  df_src.drop(df_src[df_src.model==fix_key].index, inplace=True)\n",
    "  df_src = pd.concat([df_src, df])\n",
    "  return df_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_seed = None\n",
    "\n",
    "df_dict = defaultdict(list)\n",
    "for model_key, model_df in speed_acc_data_df.groupby('model'):\n",
    "  for td_lambda, td_df in model_df.groupby('td_lambda'):\n",
    "    for q, q_df in td_df.groupby('q'):\n",
    "      if single_seed is not None:\n",
    "        q_df = q_df[q_df.seed.str.contains(f\"_{single_seed}\")]\n",
    "      acc_mean = np.mean(q_df.acc)\n",
    "      n = len(q_df)\n",
    "      acc_sem = np.std(q_df.acc) / np.sqrt(n)\n",
    "      timestep_mean = np.mean(q_df.timesteps)\n",
    "      timestep_sem = np.std(q_df.timesteps) / np.sqrt(n)\n",
    "\n",
    "      df_dict['model'].append(model_key)\n",
    "      df_dict['q'].append(q)\n",
    "      df_dict['acc_mean'].append(acc_mean)\n",
    "      df_dict['acc_sem'].append(acc_sem)\n",
    "      df_dict['timestep_mean'].append(timestep_mean)\n",
    "      df_dict['timestep_sem'].append(timestep_sem)\n",
    "      df_dict['td_lambda'].append(td_lambda)\n",
    "      df_dict['n'].append(n)\n",
    "q_stat_df = pd.DataFrame(df_dict)\n",
    "\n",
    "try:\n",
    "  q_stat_df = fix_df(q_stat_df, 'cascaded_seq__serial')\n",
    "except:\n",
    "  print(\"Exception!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-milton",
   "metadata": {},
   "source": [
    "## Speed Accuracy Tradeoff Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_best_lambdas(stats_df, sdn_TD1=True):\n",
    "  best_lambdas = {}\n",
    "  for model_key, model_df in stats_df.groupby('model'):\n",
    "    model_df = model_df[model_df.ic==model_df.ic.max()] \n",
    "    model_df = model_df[model_df.acc==model_df.acc.max()]\n",
    "    best_lambda = model_df.iloc[0].td_lambda\n",
    "    if sdn_TD1:\n",
    "      if 'cascaded__serial' in model_key:\n",
    "        best_lambdas[model_key] = 'td(1.0)'\n",
    "      elif 'cascaded__parallel' in model_key:\n",
    "        best_lambdas[model_key] = 'td(0.0)'\n",
    "      else:\n",
    "        best_lambdas[model_key] = best_lambda\n",
    "    else:\n",
    "      best_lambdas[model_key] = best_lambda\n",
    "  return best_lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_legend_order(handles, labels):\n",
    "  order = [\n",
    "      'CascadedTD', \n",
    "      'CascadedTD-MultiHead', \n",
    "      'CascadedCE',\n",
    "      'SerialTD', \n",
    "      'SerialTD-MultiHead', \n",
    "      'SerialCE',\n",
    "  ]\n",
    "  idxs = []\n",
    "  for key in order:\n",
    "    if key in labels:\n",
    "      idx = labels.index(key)\n",
    "      idxs.append(idx)\n",
    "  handles = list(np.array(handles)[idxs])\n",
    "  labels = list(np.array(labels)[idxs])\n",
    "  return handles, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_EBARS = True\n",
    "LINEWIDTH = 3\n",
    "AXIS_LBL_FONTSIZE = 16\n",
    "LEGEND_FONTSIZE = 16\n",
    "TICK_FONTSIZE = 14\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "# Compute best lambdas\n",
    "best_lambdas = compute_best_lambdas(stats_df)\n",
    "\n",
    "y_dfs = []\n",
    "for model_key, model_df in q_stat_df.groupby('model'):\n",
    "  best_lambda = best_lambdas[model_key]\n",
    "  model_df = model_df[model_df.td_lambda==best_lambda]\n",
    "  y_dfs.append(model_df)\n",
    "q_stat_df_fixed = pd.concat(y_dfs)\n",
    "\n",
    "max_flop = np.max(q_stat_df_fixed.timestep_mean)\n",
    "\n",
    "for i, (model_key, model_df) in enumerate(q_stat_df_fixed.groupby('model')):\n",
    "  linestyle = '--' if 'multiple_fcs' in model_key else '-'\n",
    "  if 'cascaded_seq' in model_key:\n",
    "    linestyle = 'dotted'\n",
    "  try:\n",
    "    color_i = model_colors[model_key]\n",
    "  except:\n",
    "    color_i = model_colors[\"cascaded__parallel__multiple_fcs\"]\n",
    "  \n",
    "  timestep_vals = np.array(list(model_df.timestep_mean))\n",
    "  \n",
    "  sorted_idxs = np.argsort(timestep_vals)\n",
    "  timestep_vals = timestep_vals[sorted_idxs]\n",
    "  acc_vals = np.array(list(model_df.acc_mean))[sorted_idxs]\n",
    "  sem_error = np.array(list(model_df.acc_sem))[sorted_idxs]\n",
    "    \n",
    "  td_lambda_lbl = model_df.iloc[0].td_lambda.replace('td(', 'TD(')\n",
    "  try:\n",
    "    label = MODEL_LBL_LOOKUP[model_key]\n",
    "  except:\n",
    "    label = \"cascaded__scheme_1__multiple_fcs\"\n",
    "  plt.plot(timestep_vals, acc_vals, label=label, \n",
    "           linewidth=LINEWIDTH, c=color_i, linestyle=linestyle)\n",
    "  \n",
    "  if SHOW_EBARS:\n",
    "    lower_b = np.array(acc_vals) - np.array(sem_error)\n",
    "    upper_b = np.array(acc_vals) + np.array(sem_error)\n",
    "    plt.fill_between(timestep_vals, lower_b, upper_b, alpha=0.075, color=color_i)\n",
    "  \n",
    "plt.xlabel('Timesteps', fontsize=AXIS_LBL_FONTSIZE)\n",
    "plt.ylabel('Accuracy (%)', fontsize=AXIS_LBL_FONTSIZE)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.tick_params(axis='both', which='major', labelsize=TICK_FONTSIZE)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles, labels = fix_legend_order(handles, labels)\n",
    "ax.legend(handles=handles, labels=labels)\n",
    "                   \n",
    "final_fig_path = fig_path\n",
    "if 'metacog_df' in globals() and _SHOW_METACOG:\n",
    "  final_fig_path = final_fig_path.replace('.pdf', '_with_metacog.pdf')\n",
    "  metacog_df = metacog_df.sort_values('mean_time')\n",
    "  metacog_time = metacog_df.mean_time\n",
    "  metacog_acc = metacog_df.mean_correct * 100\n",
    "  metacog_color = np.array((255, 148, 77)) / 255.\n",
    "  metacog_ls = (0, (3, 1, 1, 1))\n",
    "  plt.plot(metacog_time, metacog_acc, \n",
    "           linewidth=LINEWIDTH, color=metacog_color, linestyle=metacog_ls)\n",
    "  metacog_patch = Line2D([0], [0], color=metacog_color, lw=LINEWIDTH, \n",
    "                         linestyle=metacog_ls, label='MetaCog GRU-RNN')\n",
    "\n",
    "  handles += [metacog_patch]\n",
    "\n",
    "legend = ax.legend(handles=handles, frameon=False, loc='center left', \n",
    "                   bbox_to_anchor=(1., 0.5), prop={'size': LEGEND_FONTSIZE})\n",
    "fig.subplots_adjust(right=0.9)\n",
    "  \n",
    "plt.savefig(final_fig_path, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(final_fig_path.replace('pdf', 'png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-optics",
   "metadata": {},
   "source": [
    "### Print Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (model_key, model_df) in enumerate(q_stat_df_fixed.groupby('model')):\n",
    "  label = MODEL_LBL_LOOKUP[model_key]\n",
    "  final_df = model_df[model_df.timestep_mean==model_df.timestep_mean.max()].iloc[0]\n",
    "  acc = final_df.acc_mean\n",
    "  sem = final_df.acc_sem\n",
    "  print(f\"{label}: {acc:0.2f}% +/- {sem:0.2f} (n={final_df.n})\")\n",
    "  \n",
    "  xp = list(model_df.acc_mean)\n",
    "  fp = list(model_df.timestep_mean)\n",
    "  t_for_50_perc = np.interp(50, xp, fp)\n",
    "  print(f\"{label}: 50% accuracy @ timestep {t_for_50_perc:0.2f}\")\n",
    "  print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
